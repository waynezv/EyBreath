{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Identification with Ey and Breath\n",
    "> Summary\n",
    "This project explores the possibilities and the methodologies to identify speakers' identities from short recordings of *`Ey` sounds* and *`breath`es*. We construct two neural networks to capture the features of sampled recordings and to predict the speaker identities. One is `conv_net + lstm`, and the other is `conv_net + tdnn`.\n",
    "===\n",
    "\n",
    "## Goals\n",
    "Our goals are\n",
    "1. extract features from recordings;\n",
    "2. find ways to predict speaker's identity using these features, and prove their effectiveness.\n",
    "\n",
    "## Backgrounds\n",
    "\n",
    "## Methods\n",
    "Looking at the `spectrogram` of the recordings, we observe that\n",
    "\n",
    "[comment]: # ([pic_spectrogram]: ./drafts/images/spectrogram.jpg \"Spectrogram\")\n",
    "1. different speakers have different harmonics and patterns;\n",
    "2. a speaker having a higher pitch would have similar patterns with a different person in `log-spectrogram`;\n",
    "3. the spectrograms of a same person with the same sounds have varied lengths (time durations).\n",
    "\n",
    "In order to account for the scale invariance and the shift invariance, we propose\n",
    "1. use `constant Q transform` to extract spectrograms from recordings;\n",
    "2. use `convolution network` to extract features along the frequencies;\n",
    "3. use `recurrent network` (Long Short-Term Memory in this task) / `Time Delay Neural Network` (TDNN) to capture the dynamics along the time steps.\n",
    "\n",
    "### Constant Q transform\n",
    "It is closely relate to the Fourier transfrom. It has series of logarithmically spaced filters, making it suitable for analysing music signals. The key components of constant Q transform are\n",
    "1. The center frequencies are $f_k = 2^{\\frac{k}{b}}f_0$, where $b$ is the number of filters per oactave, $k = 1, \\ldots, K$, and the total number of frequency bins $K = b\\log_2(\\frac{f_{max}}{f_0})$.\n",
    "2. The bandwidth of the $k$-th filter is $\\delta f_k = f_{k+1} - f_{k} = f_k (2^{\\frac{1}{b}} - 1)$. The ratio of frequency to bandwidth is constant $Q = \\frac{f_k}{\\delta f_k} = \\frac{1}{(2^{\\frac{1}{b}} - 1)}$, and hence the notation `constant Q`.\n",
    "3. The window length for the $k$-th frequency bin is $N_k = Q\\frac{f_s}{f_k}$, where $f_s$ is the sampling frequency.\n",
    "4. Finally, the constant Q transform\n",
    "$$ x[k] = \\frac{1}{N_k}\\sum_{n=0}^{N_k-1}{x[n]w_{N_k}[n]}e^{-j2\\pi Qn/N_k}$$\n",
    "\n",
    "### Long Short-Term Memory\n",
    "The LSTM is a variant of Recurrent Neural Networks (RNN). It itself has many variants too. It was originally proposed by `Sepp Hochreiter and JÃ¼rgen Schmidhuber` in 1997, and then is used to effectively model contextual sequences. It is designed with internal memory cells and controlling gates to selectively memorize states and fight vanishing or exploding gradients. See the structure of a LSTM cell,![lstm_cell](./drafts/images/lstm_cell.png) and a sequence of cell units ![lstm_seq](./drafts/images/lstm_seq.png)\n",
    "\n",
    "####  Forward propagation\n",
    "The forward pass\n",
    "at time $t$\n",
    "1. Forget gate: $f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) = \\sigma(\\hat{f}^t)$\n",
    "2. Input gate: $i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) = \\sigma(\\hat{i}^t)$\n",
    "and candidate input: $a_t = tanh(W_c x_t + U_c h_{t-1} + b_c) = tanh(\\hat{a}^t)$\n",
    "3. Cell state update: $C_t = f_t C_{t-1} + i_t a_t$\n",
    "4. Output gate: $o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) = \\sigma(\\hat{o}^t)$\n",
    "\n",
    "Let $z^t = [\\hat{a}^t\\, \\hat{i}^t\\, \\hat{f}^t\\, \\hat{o}^t]$\n",
    "5. Hidden output: $h_t = o_t tanh(C_t)$\n",
    "\n",
    "#### Backpropagation through time\n",
    "The error is backpropagated and the gradients are calculated to update the weights. Different with multilayer perception networks, we need to backpropagate the gradients through time (BPTT). ![bptt](./drafts/images/bptt.png) Using the chain rule, we unroll the network at each time step and compute the partial derivatives of the error wrt each paramter participated in. \n",
    "1. $$\\frac{\\partial E}{\\partial o_i^t} = \\frac{\\partial E}{\\partial h_i^t}\\frac{\\partial h_i^t}{o_i^t} = \\delta h_i^t tanh(c_i^t)$$\n",
    "$$\\delta o^t = \\delta h^t tanh(c^t)$$\n",
    "2. $$\\frac{\\partial E}{\\partial c_i^t} = \\frac{\\partial E}{\\partial h_i^t}\\frac{\\partial h_i^t}{c_i^t} = \\delta h_i^t o_i^t (1 - tanh^2(c_i^t))$$\n",
    "$$\\delta c^t += \\delta h^t o^t (1 - tanh^2(c^t))$$\n",
    "3. $$\\frac{\\partial E}{\\partial i_i^t} = \\frac{\\partial E}{\\partial c_i^t}\\frac{\\partial c_i^t}{i_i^t} = \\delta c_i^t a_i^t$$\n",
    "$$\\delta i^t = \\delta c^t a^t$$\n",
    "4. $$\\frac{\\partial E}{\\partial f_i^t} = \\frac{\\partial E}{\\partial c_i^t}\\frac{\\partial c_i^t}{f_i^t} = \\delta c_i^t c_i^{t-1}$$\n",
    "$$\\delta f^t = \\delta c^t c^{t-1}$$\n",
    "5. $$\\frac{\\partial E}{\\partial a_i^t} = \\frac{\\partial E}{\\partial c_i^t}\\frac{\\partial c_i^t}{a_i^t} = \\delta c_i^t i_i^t$$\n",
    "$$\\delta a^t = \\delta c^t i^t$$\n",
    "6. $$\\frac{\\partial E}{\\partial c_i^{t-1}} = \\frac{\\partial E}{\\partial c_i^t}\\frac{\\partial c_i^t}{c_i^{t-1}} = \\delta c_i^t f_i^t$$\n",
    "$$\\delta c^{t-1} = \\delta c^t f^t$$\n",
    "7. $$\\delta \\hat{a}^t = \\delta a^t (1 - tanh^2(\\hat{a}^t))$$\n",
    "$$\\delta \\hat{i}^t = \\delta i^t i^t (1 - i^t)$$\n",
    "$$\\delta \\hat{f}^t = \\delta f^t f^t (1 - f^t)$$\n",
    "$$\\delta \\hat{o}^t = \\delta o^t o^t (1 - o^t)$$\n",
    "$$\\delta z^t = [\\delta \\hat{a}^t\\, \\delta \\hat{i}^t\\, \\delta \\hat{f}^t\\, \\delta \\hat{o}^t]$$\n",
    "8. $$\\delta I^t = W \\delta z^t,$$ where $I^t = [x^t\\, h^{t-1}]$.\n",
    "$$\\delta W^t = \\delta z^t I^t$$\n",
    "\n",
    "Given $x = [x_1, \\ldots, x_T]$, $\\delta W = \\sum_{t=1}^{T} \\delta W^t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "### Data Preparation\n",
    "#### Feature extraction\n",
    "* num_bin_per_octave = 48\n",
    "* fs = 44100 Hz\n",
    "* fmin = 27.5 Hz\n",
    "* fmax = fs/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "We selected from the dataset the portion in which each speaker has more than **100** samples. This contains **12942** instances out of a total **25368** instances, corresponding to **53** speakers out of a total **805** speakers. Then, for each speaker, we select 70% as training set, 20% as validation set, and 10% as test set. The datasets are the shuffled. The codes are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data preparation for speaker identification with Ey Breath.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from theano import config\n",
    "\n",
    "speaker_dict = dict() # store 'speaker_id (string) : id (integer)' pairs\n",
    "instance_dict = dict() # store 'speaker_id (string) : instances (list of\n",
    "    # instance objects)' pairs\n",
    "count_dict = dict() # store 'speaker_id (string) : counter (integer)' pairs\n",
    "instance_collection = [] # collection of instances\n",
    "\n",
    "# TODO:\n",
    "def normalization():\n",
    "    \"\"\"\n",
    "    Normalize input features.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class Instance:\n",
    "    \"\"\"\n",
    "    One training instance.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_id, speaker_id, featvec):\n",
    "        self.file_id = file_id\n",
    "        self.speaker_id = speaker_id\n",
    "        self.featvec = featvec\n",
    "\n",
    "def read_instance(data_path, filename, spk_smpl_thrd = 100, wrt_dict = False):\n",
    "    \"\"\"\n",
    "    Read data as instances.\n",
    "\n",
    "    :data_path: (string) path containing data to be processed\n",
    "    :filename: (string) file containing names of interested files\n",
    "    :spk_smpl_thrd: (integer) only speaker that has samples exceeds\n",
    "    this number is counted in\n",
    "    :wrt_dict: (bool) write speaker_dict to file or not\n",
    "    \"\"\"\n",
    "\n",
    "    filelist = [l for l in open(filename)] # all files in filename\n",
    "    interest_filelist = [''.join([ l.split()[0].split('.')[0], '.txt' ])\n",
    "                         for l in filelist] # filenames of interest\n",
    "    num_files = len(filelist)\n",
    "    for i in xrange(num_files):\n",
    "        name = filelist[i]\n",
    "        sspk_id = name.split()[1]\n",
    "        if sspk_id in speaker_dict: # if speaker in dict\n",
    "            count_dict[sspk_id] += 1 # add counter\n",
    "            speaker_id = speaker_dict[sspk_id] # get mapped id\n",
    "\n",
    "        else: # if speaker shows up first time\n",
    "            count_dict[sspk_id] = 1 # set counter\n",
    "            speaker_dict[sspk_id] = len(speaker_dict)\n",
    "            speaker_id = speaker_dict[sspk_id]\n",
    "            instance_dict[sspk_id] = [] # init speaker instance list\n",
    "\n",
    "        file = interest_filelist[i]\n",
    "        featvec = np.asarray([\n",
    "            line.strip().split(',')\n",
    "            for line in open(os.path.join(data_path, file))\n",
    "        ], dtype = 'float')\n",
    "\n",
    "        instance_dict[sspk_id].append( Instance(file, speaker_id, featvec) )\n",
    "\n",
    "    enrolled_ins_count = 0 # counter for enrolled instances\n",
    "    sid = 0\n",
    "    for k in count_dict: # for each speaker\n",
    "        if count_dict[k] >= spk_smpl_thrd: # if exceeds threshold\n",
    "            ins_list = instance_dict[k]\n",
    "            enrolled_ins_count += len(ins_list)\n",
    "            for r in ins_list:\n",
    "                ins_list[r].speaker_id = int(sid) # reprog speaker id\n",
    "            instance_collection.append(ins_list) # add instances to collection\n",
    "            sid += 1\n",
    "    num_enrolled_spk = len(instance_collection) # number of enrolled speakers\n",
    "\n",
    "    if wrt_dict:\n",
    "        write_dict(speaker_dict, ''.join([filename, '_consq_speaker_dict.txt']))\n",
    "        print('Saved speaker_dict to file: ', filename + '_consq_speaker_dict.txt')\n",
    "\n",
    "    print('processed ', filename, ', total ', num_files, ' files')\n",
    "    print('total ', len(speaker_dict), ' speakers')\n",
    "    print('totally enrolled ', enrolled_ins_count, ' instances, ',\n",
    "          num_enrolled_spk, ' speakers.')\n",
    "\n",
    "    return num_enrolled_spk, enrolled_ins_count\n",
    "\n",
    "def write_dict(speaker_dict, filename):\n",
    "    out = ''\n",
    "    for k in speaker_dict:\n",
    "        out += str(k) + ',' + str(speaker_dict[k]) + '\\n' # k, v\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(out)\n",
    "\n",
    "def prepare_data(ins_list, idx_list):\n",
    "    \"\"\"\n",
    "    Format data to network required form.\n",
    "    :ins_list: list of instances\n",
    "    :idx_list: list of indices\n",
    "    <- [X (list of 4d tensors), y (list of integers)]\n",
    "    \"\"\"\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    for idx in idx_list:\n",
    "        ins = ins_list[idx]\n",
    "        spk_id = ins.speaker_id\n",
    "        y.append(spk_id)\n",
    "        ins_vec = ins.featvec\n",
    "        nrow, ncol = ins_vec.shape\n",
    "        inp = np.asarray(ins_vec, dtype = config.floatX).reshape(\n",
    "            (1, 1, nrow, ncol))\n",
    "        X.append(inp)\n",
    "\n",
    "    return zip(X, y)\n",
    "\n",
    "def load_data(dpath, filename, shuffle = False, spk_smpl_thrd = 100):\n",
    "    \"\"\"\n",
    "    Load data.\n",
    "    <- train_set, val_set, test_set: [X (list of 4d tensors), y (list of integers)]\n",
    "    \"\"\"\n",
    "    # Read instances from constant Q features\n",
    "    num_enrolled_spk, enrolled_ins_count = read_instance(dpath, filename, spk_smpl_thrd)\n",
    "\n",
    "    # Split dataset\n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    test_set = []\n",
    "    split_ratio = (0.7, 0.2, 0.1)\n",
    "    for i in xrange(num_enrolled_spk):\n",
    "        ins_list = instance_collection[i]\n",
    "        num_ins = len(ins_list)\n",
    "        num_trns = int(np.floor(num_ins * split_ratio[0]))\n",
    "        num_vals = int(np.floor(num_ins * split_ratio[1]))\n",
    "        num_devs = int(num_ins - num_trns - num_vals)\n",
    "\n",
    "        smpl_list = np.arange(num_ins)\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(smpl_list)\n",
    "\n",
    "        trn_list = smpl_list[: num_trns]\n",
    "        val_list = smpl_list[num_trns : num_trns + num_vals]\n",
    "        dev_list = smpl_list[num_trns + num_vals : num_ins]\n",
    "\n",
    "        xy_trn = prepare_data(ins_list, trn_list)\n",
    "        xy_val = prepare_data(ins_list, val_list)\n",
    "        xy_dev = prepare_data(ins_list, dev_list)\n",
    "\n",
    "        for x, y in xy_trn:\n",
    "            train_set.append((x, y))\n",
    "        for x, y in xy_val:\n",
    "            val_set.append((x, y))\n",
    "        for x, y in xy_dev:\n",
    "            test_set.append((x, y))\n",
    "\n",
    "    if shuffle:\n",
    "        tlf = np.random.permutation(len(train_set))\n",
    "        vlf = np.random.permutation(len(val_set))\n",
    "        dlf = np.random.permutation(len(test_set))\n",
    "        train_set = [train_set[s] for s in tlf]\n",
    "        val_set = [val_set[s] for s in vlf]\n",
    "        test_set = [test_set[s] for s in dlf]\n",
    "\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object cannot be interpreted as an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e1905301337b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./feat_constq'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpath\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'ey'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./ey.interested'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspk_smpl_thrd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-9dca113218eb>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(dpath, filename, shuffle, spk_smpl_thrd)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \"\"\"\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# Read instances from constant Q features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mnum_enrolled_spk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menrolled_ins_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspk_smpl_thrd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Split dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9dca113218eb>\u001b[0m in \u001b[0;36mread_instance\u001b[0;34m(data_path, filename, spk_smpl_thrd, wrt_dict)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0menrolled_ins_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mins_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mins_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeaker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reprog speaker id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0minstance_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add instances to collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0msid\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object cannot be interpreted as an index"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "dpath = './feat_constq'\n",
    "train_set, val_set, test_set = load_data(os.path.join(dpath ,'ey'), './ey.interested', shuffle = True, spk_smpl_thrd = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Structure\n",
    "The structures of `conv_net + lstm` and `conv_net + tdnn`\n",
    "![lstm](./drafts/images/lstm.png)\n",
    "![lstm](./drafts/images/tdnn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Core layers\n",
    "# LSTM\n",
    "class LSTMBuilder:\n",
    "    \"\"\"\n",
    "    Net builder for LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, inp, prefix, rand_scheme = 'standnormal'):\n",
    "        \"\"\"\n",
    "        :inp: (num_time_steps, num_samples, num_embedding_size)\n",
    "\n",
    "        TODO:\n",
    "        1. Batch & mask\n",
    "        2. Output dim\n",
    "        \"\"\"\n",
    "\n",
    "        n_tstep, n_samp, embd_size = inp.shape[0], inp.shape[1], inp.shape[2]\n",
    "        embd_size = 15\n",
    "        self.n_tstep = n_tstep\n",
    "        self.n_samp = n_samp\n",
    "        self.embd_size = embd_size\n",
    "\n",
    "        W_val = np.concatenate([ortho_weight(embd_size),\n",
    "                            ortho_weight(embd_size),\n",
    "                            ortho_weight(embd_size),\n",
    "                            ortho_weight(embd_size)], axis=1)\n",
    "        W = T.shared(value = W_val, name = prefix + '_W', borrow = True)\n",
    "        U_val = np.concatenate([ortho_weight(embd_size),\n",
    "                            ortho_weight(embd_size),\n",
    "                            ortho_weight(embd_size),\n",
    "                            ortho_weight(embd_size)], axis=1)\n",
    "        U = T.shared(value = U_val, name = prefix + '_U', borrow = True)\n",
    "        b_val = np.zeros((4 * embd_size,), dtype = config.floatX)\n",
    "        b = T.shared(value = b_val, name = prefix + '_b', borrow = True)\n",
    "\n",
    "        self.input = inp\n",
    "        self.W = W\n",
    "        self.U = U\n",
    "        self.b = b\n",
    "\n",
    "        # Parallelize: process one batch: n_samp per n_tstep.\n",
    "        self.inp_t = tensor.dot(self.input, self.W) + self.b\n",
    "        '''\n",
    "        rval, updates = T.scan(self._step,\n",
    "                               sequences    = self.inp_t,\n",
    "                               outputs_info = [\n",
    "                                   tensor.zeros_like(self.inp_t),\n",
    "                                   tensor.zeros_like(self.inp_t)\n",
    "                                   ],\n",
    "                               name = prefix + '_layers',\n",
    "                               n_steps = n_tstep)\n",
    "        '''\n",
    "\n",
    "        rval, updates = T.scan(self._step,\n",
    "                               sequences    = self.inp_t,\n",
    "                               outputs_info = [\n",
    "                                   tensor.alloc(\n",
    "                                       np.asarray(0., dtype = config.floatX),\n",
    "                                       n_samp,\n",
    "                                       embd_size\n",
    "                                       ),\n",
    "                                   tensor.alloc(\n",
    "                                       np.asarray(0., dtype = config.floatX),\n",
    "                                       n_samp,\n",
    "                                       embd_size\n",
    "                                       )\n",
    "                                   ],\n",
    "                               name = prefix + '_layers',\n",
    "                               n_steps = n_tstep)\n",
    "\n",
    "        self.output = rval[0] # h: (n_tstep, n_samp, embd_size)\n",
    "\n",
    "        self.f = T.function([inp], self.output, name = 'f_' + prefix)\n",
    "        self.params = [self.W, self.U, self.b]\n",
    "\n",
    "    def _step(self, x_, h_, c_):\n",
    "        \"\"\"\n",
    "        :x_: W * x_t\n",
    "        :h_: h_(t-1)\n",
    "        :c_: c_(t-1)\n",
    "        \"\"\"\n",
    "        preact = x_ + tensor.dot(h_, self.U)\n",
    "\n",
    "        # Gates, outputs, states\n",
    "        i = tensor.nnet.sigmoid(self._slice(preact, 0, self.embd_size))\n",
    "        f = tensor.nnet.sigmoid(self._slice(preact, 1, self.embd_size))\n",
    "        o = tensor.nnet.sigmoid(self._slice(preact, 2, self.embd_size))\n",
    "        c = tensor.tanh(self._slice(preact, 3, self.embd_size))\n",
    "\n",
    "        # States update\n",
    "        c = f * c_ + i * c\n",
    "\n",
    "        # Hidden updat\n",
    "        h = o * tensor.tanh(c)\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    def _slice(self, _x, n, dim):\n",
    "        if _x.ndim == 3:\n",
    "            return _x[:, :, n * dim:(n + 1) * dim]\n",
    "        return _x[:, n * dim:(n + 1) * dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvolutionBuilder:\n",
    "    \"\"\"\n",
    "    Net builder for Convolution layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, inp, filter_shape, prefix, stride = (1, 1),\n",
    "            W = None, b = None, rand_scheme = 'standnormal'):\n",
    "        n_samp, n_ch, n_row, n_col = inp.shape[0], inp.shape[1], inp.shape[2], inp.shape[3]\n",
    "        n_filt, n_in_featmap, filt_hgt, filt_wdh = filter_shape\n",
    "\n",
    "        if W == None:\n",
    "            if rand_scheme == 'uniform':\n",
    "                fan_in = n_in_featmap * filt_hgt * filt_wdh\n",
    "                fan_out = n_filt * filt_hgt * filt_wdh // np.prod(stride)\n",
    "                W_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "                W_val = np.asarray(\n",
    "                    rng.uniform(\n",
    "                        low  = -W_bound,\n",
    "                        high = W_bound,\n",
    "                        size = filter_shape\n",
    "                    ),\n",
    "                    dtype = config.floatX\n",
    "                )\n",
    "            elif rand_scheme == 'standnormal':\n",
    "                W_val = np.asarray(\n",
    "                    rng.randn(n_filt, n_in_featmap, filt_hgt, filt_wdh),\n",
    "                    dtype = config.floatX\n",
    "                )\n",
    "            elif rand_scheme == 'orthogonal':\n",
    "                pass\n",
    "            elif rand_scheme == 'identity':\n",
    "                pass\n",
    "            W = T.shared(value = W_val, name = prefix + '_W', borrow = True)\n",
    "\n",
    "        if b == None:\n",
    "            b_val = np.zeros((n_filt,), dtype = config.floatX)\n",
    "            b = T.shared(value = b_val, name = prefix + '_b', borrow = True)\n",
    "\n",
    "        self.input = inp\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.output = conv2d(input = inp, filters = self.W, filter_shape = filter_shape,\n",
    "                          subsample = stride) + self.b.dimshuffle('x', 0, 'x', 'x')\n",
    "\n",
    "        self.f = T.function([inp], self.output, name = 'f_' + prefix)\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dense\n",
    "class DenseBuilder:\n",
    "    \"\"\"\n",
    "    Net builder for Dense layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, inp, in_dim, out_dim, prefix,\n",
    "                 W = None, b = None, rand_scheme = 'standnormal'):\n",
    "        n_samp, m = inp.shape[0], inp.shape[1]\n",
    "\n",
    "        if W == None:\n",
    "            if rand_scheme == 'uniform':\n",
    "                W_val = np.asarray(\n",
    "                    rng.uniform(\n",
    "                        low  = -np.sqrt(6. / (m + out_dim)),\n",
    "                        high =  np.sqrt(6. / (m + out_dim)),\n",
    "                        size = (m, out_dim)\n",
    "                    ),\n",
    "                    dtype = config.floatX\n",
    "                )\n",
    "            elif rand_scheme == 'standnormal':\n",
    "                W_val = np.asarray(\n",
    "                    rng.randn(in_dim, out_dim),\n",
    "                    dtype = config.floatX\n",
    "                )\n",
    "            elif rand_scheme == 'orthogonal':\n",
    "                pass\n",
    "            elif rand_scheme == 'identity':\n",
    "                pass\n",
    "            W = T.shared(value = W_val, name = prefix + '_W', borrow = True)\n",
    "\n",
    "        if b == None:\n",
    "            b_val = np.zeros((out_dim,), dtype = config.floatX)\n",
    "            b = T.shared(value = b_val, name = prefix + '_b', borrow = True)\n",
    "\n",
    "        self.input = inp\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.output = tensor.dot(self.input, W) + b\n",
    "\n",
    "        self.f = T.function([inp], self.output, name = 'f_' + prefix)\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "* Loss: categorical crossentropy\n",
    "* Optimizer: RMSProp\n",
    "RMSProp is a gradient-based optimization algorithm. It is similar to Adagrad, but introduces an additional decay term to counteract Adagradâs rapid decrease in learning rate.\n",
    "\n",
    "The math\n",
    "$$g_t = \\nabla_\\theta J( \\theta_t )$$\n",
    "$$E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g_t^2$$\n",
    "$$RMS[g]_{t} = \\sqrt{E[g^2]_t + \\epsilon}$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\dfrac{\\eta}{RMS[g]_{t}}f_t$$\n",
    "\n",
    "We set $\\eta = 1e^{-4}$, $\\gamma = 0.95$, and $\\epsilon = 10e^{-8}$.\n",
    "\n",
    "* Batch size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementation of RMSProp\n",
    "def rmsprop(lr, params, grads, x, y, cost):\n",
    "    \"\"\"\n",
    "    A variant of SGD that scales the step size by running average of the\n",
    "    recent step norms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Theano SharedVariable\n",
    "        Initial learning rate\n",
    "    pramas: Theano SharedVariable\n",
    "        Model parameters\n",
    "    grads: Theano variable\n",
    "        Gradients of cost w.r.t to parameres\n",
    "    x: Theano variable\n",
    "        Model inputs\n",
    "    y: Theano variable\n",
    "        Targets\n",
    "    cost: Theano variable\n",
    "        Objective fucntion to minimize\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For more information, see [Hint2014]_.\n",
    "\n",
    "    .. [Hint2014] Geoff Hinton, *Neural Networks for Machine Learning*,\n",
    "       lecture 6a,\n",
    "       http://cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    zipped_grads = [T.shared(p.get_value() * np.asarray(0., dtype=config.floatX),\n",
    "                                  name='%s_grad' % str(k))\n",
    "                    for k, p in zip(range(len(params)), params)]\n",
    "    running_grads = [T.shared(p.get_value() * np.asarray(0., dtype=config.floatX),\n",
    "                                  name='%s_rgrad' % str(k))\n",
    "                    for k, p in zip(range(len(params)), params)]\n",
    "    running_grads2 = [T.shared(p.get_value() * np.asarray(0., dtype=config.floatX),\n",
    "                                  name='%s_rgrad2' % str(k))\n",
    "                      for k, p in zip(range(len(params)), params)]\n",
    "\n",
    "    zgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "    rgup = [(rg, 0.95 * rg + 0.05 * g) for rg, g in zip(running_grads, grads)]\n",
    "    rg2up = [(rg2, 0.95 * rg2 + 0.05 * (g ** 2))\n",
    "             for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "    f_grad_shared = T.function([x, y], cost,\n",
    "                               updates=zgup + rgup + rg2up,\n",
    "                               name='rmsprop_f_grad_shared')\n",
    "\n",
    "    updir = [T.shared(p.get_value() * np.asarray(0., dtype=config.floatX),\n",
    "                      name='%s_updir' % k)\n",
    "             for k, p in zip(range(len(params)), params)]\n",
    "    updir_new = [(ud, 0.9 * ud - 1e-4 * zg / tensor.sqrt(rg2 - rg ** 2 + 1e-4))\n",
    "                 for ud, zg, rg, rg2 in zip(updir, zipped_grads, running_grads,\n",
    "                                            running_grads2)]\n",
    "    param_up = [(p, p + udn[1])\n",
    "                for p, udn in zip(params, updir_new)]\n",
    "    f_update = T.function([lr], [], updates=updir_new + param_up,\n",
    "                               on_unused_input='ignore',\n",
    "                               name='rmsprop_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Tricks\n",
    "1. Weights Initialization: The weights are initialized orthogonally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Orthogonal initialization\n",
    "def ortho_weight(ndim):\n",
    "    W = np.random.randn(ndim, ndim)\n",
    "    u, s, v = np.linalg.svd(W)\n",
    "    return u.astype(config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Parallel & GPU\n",
    "3. Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "(Run 200 epoches)\n",
    "\n",
    "| Network | CONV + LSTM | CONV + TDNN |\n",
    "| ------- |------------:| -----------:|\n",
    "| Ey      |  81.1299%   |\n",
    "| Breath  |  86.6417%   |\n",
    "\n",
    "(Still improving)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "~~1. run breath~~\n",
    "2. finish tdnn\n",
    "3. examine code\n",
    "4. tune: structure, params, optimizers\n",
    "5. grant git access\n",
    "6. look into literature\n",
    "7. write-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Thoughts "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
