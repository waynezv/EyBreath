{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Identification with Ey and Breath\n",
    "> Summary\n",
    "This project explores the possibilities and the methodologies to identify speakers' identities from short recordings of *`Ey` sounds* and *`breath`es*. We construct two neural networks to capture the features of sampled recordings and to predict the speaker identities. One is `conv_net + lstm`, and the other is `conv_net + tdnn`.\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "Our goals are\n",
    "1. extract features from recordings;\n",
    "2. find ways to predict speaker's identity using these features, and prove their effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "Looking at the `spectrogram` of the recordings, we observe that\n",
    "\n",
    "[comment]: # ([pic_spectrogram]: ./drafts/images/spectrogram.jpg \"Spectrogram\")\n",
    "1. different speakers have different harmonics and patterns;\n",
    "2. a speaker having a higher pitch would have similar patterns with a different person in `log-spectrogram`;\n",
    "3. the spectrograms of a same person with the same sounds have varied lengths (time durations).\n",
    "\n",
    "In order to account for the scale invariance and the shift invariance, we propose\n",
    "1. use `constant Q transform` to extract spectrograms from recordings;\n",
    "2. use `convolution network` to extract features along the frequencies;\n",
    "3. use `recurrent network` (Long Short-Term Memory in this task) / `Time Delay Neural Network` (TDNN) to capture the dynamics along the time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Q transform\n",
    "It is closely relate to the Fourier transfrom. It has series of logarithmically spaced filters, making it suitable for analysing music signals. The key components of constant Q transform are\n",
    "1. The center frequencies are $f_k = 2^{\\frac{k}{b}}f_0$, where $b$ is the number of filters per oactave, $k = 1, \\ldots, K$, and the total number of frequency bins $K = b\\log_2(\\frac{f_{max}}{f_0})$.\n",
    "2. The bandwidth of the $k$-th filter is $\\delta f_k = f_{k+1} - f_{k} = f_k (2^{\\frac{1}{b}} - 1)$. The ratio of frequency to bandwidth is constant $Q = \\frac{f_k}{\\delta f_k} = \\frac{1}{(2^{\\frac{1}{b}} - 1)}$, and hence the notation `constant Q`.\n",
    "3. The window length for the $k$-th frequency bin is $N_k = Q\\frac{f_s}{f_k}$, where $f_s$ is the sampling frequency.\n",
    "4. Finally, the constant Q transform\n",
    "$$ x[k] = \\frac{1}{N_k}\\sum_{n=0}^{N_k-1}{x[n]w_{N_k}[n]}e^{-j2\\pi Qn/N_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Neural Nets\n",
    "1. Some patterns are much smaller than the whole image --> small filters, multi-filters\n",
    "2. The same patterns appear in different regions --> shared filter\n",
    "3. Subsampling the pixels will not change the object --> pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory\n",
    "The LSTM is a variant of Recurrent Neural Networks (RNN). It itself has many variants too. It was originally proposed by `Sepp Hochreiter and Jürgen Schmidhuber` in 1997, and then is used to effectively model contextual sequences. It is designed with internal memory cells and controlling gates to selectively memorize states and fight vanishing or exploding gradients. See the structure of a LSTM cell,![lstm_cell](./drafts/images/lstm_cell.png) and a sequence of cell units ![lstm_seq](./drafts/images/lstm_seq.png)\n",
    "\n",
    "####  Forward propagation\n",
    "The forward pass\n",
    "at time $t$\n",
    "1. Forget gate: $f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) = \\sigma(\\hat{f}^t)$\n",
    "2. Input gate: $i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) = \\sigma(\\hat{i}^t)$\n",
    "and candidate input: $a_t = tanh(W_c x_t + U_c h_{t-1} + b_c) = tanh(\\hat{a}^t)$\n",
    "3. Cell state update: $C_t = f_t C_{t-1} + i_t a_t$\n",
    "4. Output gate: $o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) = \\sigma(\\hat{o}^t)$\n",
    "\n",
    "Let $z^t = [\\hat{a}^t\\, \\hat{i}^t\\, \\hat{f}^t\\, \\hat{o}^t]$\n",
    "5. Hidden output: $h_t = o_t tanh(C_t)$\n",
    "\n",
    "#### Backpropagation through time\n",
    "The error is backpropagated and the gradients are calculated to update the weights. Different with multilayer perception networks, we need to backpropagate the gradients through time (BPTT). ![bptt](./drafts/images/bptt.png) Using the chain rule, we unroll the network at each time step and compute the partial derivatives of the error wrt each paramter participated in. \n",
    "1. $$\\frac{\\partial E}{\\partial o_i^t} = \\frac{\\partial E}{\\partial h_i^t}\\frac{\\partial h_i^t}{o_i^t} = \\delta h_i^t tanh(c_i^t)$$\n",
    "$$\\delta o^t = \\delta h^t tanh(c^t)$$\n",
    "2. $$\\frac{\\partial E}{\\partial c_i^t} = \\frac{\\partial E}{\\partial h_i^t}\\frac{\\partial h_i^t}{c_i^t} = \\delta h_i^t o_i^t (1 - tanh^2(c_i^t))$$\n",
    "$$\\delta c^t += \\delta h^t o^t (1 - tanh^2(c^t))$$\n",
    "3. $$\\frac{\\partial E}{\\partial i_i^t} = \\frac{\\partial E}{\\partial c_i^t}\\frac{\\partial c_i^t}{i_i^t} = \\delta c_i^t a_i^t$$\n",
    "$$\\delta i^t = \\delta c^t a^t$$\n",
    "4. $$\\frac{\\partial E}{\\partial f_i^t} = \\frac{\\partial E}{\\partial c_i^t}\\frac{\\partial c_i^t}{f_i^t} = \\delta c_i^t c_i^{t-1}$$\n",
    "$$\\delta f^t = \\delta c^t c^{t-1}$$\n",
    "5. $$\\frac{\\partial E}{\\partial a_i^t} = \\frac{\\partial E}{\\partial c_i^t}\\frac{\\partial c_i^t}{a_i^t} = \\delta c_i^t i_i^t$$\n",
    "$$\\delta a^t = \\delta c^t i^t$$\n",
    "6. $$\\frac{\\partial E}{\\partial c_i^{t-1}} = \\frac{\\partial E}{\\partial c_i^t}\\frac{\\partial c_i^t}{c_i^{t-1}} = \\delta c_i^t f_i^t$$\n",
    "$$\\delta c^{t-1} = \\delta c^t f^t$$\n",
    "7. $$\\delta \\hat{a}^t = \\delta a^t (1 - tanh^2(\\hat{a}^t))$$\n",
    "$$\\delta \\hat{i}^t = \\delta i^t i^t (1 - i^t)$$\n",
    "$$\\delta \\hat{f}^t = \\delta f^t f^t (1 - f^t)$$\n",
    "$$\\delta \\hat{o}^t = \\delta o^t o^t (1 - o^t)$$\n",
    "$$\\delta z^t = [\\delta \\hat{a}^t\\, \\delta \\hat{i}^t\\, \\delta \\hat{f}^t\\, \\delta \\hat{o}^t]$$\n",
    "8. $$\\delta I^t = W \\delta z^t,$$ where $I^t = [x^t\\, h^{t-1}]$.\n",
    "$$\\delta W^t = \\delta z^t I^t$$\n",
    "\n",
    "Given $x = [x_1, \\ldots, x_T]$, $\\delta W = \\sum_{t=1}^{T} \\delta W^t$.\n",
    "\n",
    "> Truncated BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curve of LSTM\n",
    "Hard to train!\n",
    "![train_lstm](./drafts/images/train_lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Delay Neural Network\n",
    "It works on sequential data to recognise features independent of time-shift (i.e. sequence position, shift-invariant). The input signal is augmented with delayed copies as other inputs.\n",
    "\n",
    "> Works like windows / tapers\n",
    "> TDNN: FIR filter $x[t] = \\sum_{i} \\beta_i u[t-i]$ Moving average model\n",
    "> RNN: IIR filter $x[t] = u[t] + \\sum_{i} \\alpha_i x[t-i]$ Autoregressive model\n",
    "> ![FIR_IIR](./drafts/images/fir_iir.png)\n",
    "\n",
    "Different delay mechanisms:\n",
    "1. Delay lines - short memory\n",
    "2. Decay - long memory\n",
    "3. Gamma - long-short memory\n",
    "4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "### Data Preparation\n",
    "#### Feature extraction\n",
    "* num_bin_per_octave = 48\n",
    "* fs = 44100 Hz\n",
    "* fmin = 27.5 Hz\n",
    "* fmax = fs/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Statistics\n",
    "We selected from the dataset the portion in which each speaker has more than **100** samples. For the `Ey` data, this contains **12942** instances out of a total **25368** instances, corresponding to **53** speakers out of a total **805** speakers. For the `Breath` data, this contains **9376** instances out of a total **19613** instances, corresponding to **44** speakers out of a total **725** speakers. Then, for each speaker, we select 70% as training set, 20% as validation set, and 10% as test set. The datasets are the shuffled. The codes are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Code\n",
    "```python\n",
    "import eybreath_data_prepare\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Structure\n",
    "The structures of `conv_net + lstm` and `conv_net + tdnn`\n",
    "![lstm](./drafts/images/lstm.png)\n",
    "![lstm](./drafts/images/tdnn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** Revision of structures **\n",
    ">\n",
    "> LSTM:\n",
    "> 1. use smaller filers in first convolution layer\n",
    "> 2. remove meanpooling, only take the output of the last timestep\n",
    ">\n",
    "> TDNN: add meanpooling after second convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Code\n",
    "```python\n",
    "import w_net\n",
    "class LSTMBuilder()\n",
    "class ConvolutionBuilder()\n",
    "class DenseBuilder()\n",
    "class ActivationBuilder()\n",
    "class ReshapeBuilder()\n",
    "class PoolBuilder()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "#### Loss\n",
    "softmax outputs probability simplex ->\n",
    "\n",
    "categorical cross entropy $H(p,q) = E_p[-\\log q] = H(p) + D_{KL}(p||q)$ ->\n",
    "\n",
    "KL distance (neglecting constant entropy $H(p) = E_p[-log p]$), which is actually the distance between the expected values of two negative log-likelihoods measured on the true prob $p$, in a nonparametric way. Can also be viewed in a parametric way, we want a high power (likelihood) over the hypothesized parameters. ->\n",
    "\n",
    "minimize the expected negative log-likelihood (risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers\n",
    "Let\n",
    "\n",
    "$$g_t = \\nabla_\\theta J( \\theta_t )$$\n",
    "$$E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g_t^2$$\n",
    "$$RMS[g]_{t} = \\sqrt{E[g^2]_t + \\epsilon}$$\n",
    "\n",
    "1. Stochastic gradient descent\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta J( \\theta)$$\n",
    "\n",
    "2. Momentum\n",
    "$$v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta_t)$$\n",
    "$$\\theta_{t+1} = \\theta_t - v_t$$\n",
    "\n",
    "3. Nesterov accelerated gradient\n",
    "$$v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta_t )$$\n",
    "$$\\theta_{t+1} = \\theta_t - (1 + \\gamma) v_t  + \\gamma v_{t-1}$$\n",
    "\n",
    "4. Adagrad\n",
    "$$\\theta_{t+1} = \\theta_t - \\dfrac{\\eta}{\\sqrt{G_{t} + \\epsilon}}  \\nabla_\\theta J( \\theta),$$\n",
    "where $G_t \\in \\mathbb{R}^{d \\times d}$ is a diagonal matrix where each diagonal element $G_{t,ii}$ is the sum of the squares of the gradients w.r.t. $\\theta_i$ up to time step $t$.\n",
    "\n",
    "5. Adadelta\n",
    "\n",
    "Adadelta is a gradient descent based learning algorithm that adapts the learning rate per parameter over time. It was proposed as an improvement over Adagrad, which is more sensitive to hyperparameters and may decrease the learning rate too aggressively.\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\dfrac{RMS[\\Delta \\theta]_{t-1}}{RMS[g]_{t}} \\nabla_\\theta J( \\theta_t )$$\n",
    "\n",
    "6. RMSProp\n",
    "\n",
    "RMSProp is a gradient-based optimization algorithm. It is similar to Adagrad, but introduces an additional decay term to counteract Adagrad’s rapid decrease in learning rate.\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\dfrac{\\eta}{RMS[g]_{t}}f_t$$\n",
    "\n",
    "7. Adam\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t,\\quad \\hat{m}_t = \\dfrac{m_t}{1 - \\beta^t_1}$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2,\\quad  \\hat{v}_t = \\dfrac{v_t}{1 - \\beta^t_2}$$\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training specs\n",
    "* Batch size: 1\n",
    "* $\\eta = 1e^{-4}$, $\\gamma = 0.9$, and $\\epsilon = 10e^{-8}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Code\n",
    "```python\n",
    "# Optimizer\n",
    "import w_net\n",
    "def sgd()\n",
    "def adadelta()\n",
    "def rmsprop()\n",
    "# Training procedures\n",
    "import conv_tdnn_v0.2\n",
    "def build_model()\n",
    "def train_model()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Tricks\n",
    "1. Weights Initialization: The weights are initialized orthogonally.\n",
    "\n",
    "> Code\n",
    "```python\n",
    "import w_net\n",
    "def ortho_weight()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Parallel & GPU\n",
    "3. Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Accuracy\n",
    "(Run # epoches)\n",
    "\n",
    "| Network | CONV + LSTM | CONV + TDNN |\n",
    "| ------- |------------:| -----------:|\n",
    "| Ey      |  81.1299%   |   35.12%    |\n",
    "| Breath  |  86.6417%   |   30.90%    |\n",
    "\n",
    "### Parameter size v.s. Performance\n",
    "1. Structure: LSTM\n",
    "2. Data: Ey\n",
    "\n",
    "| Parameter size | Err      |\n",
    "|:---------------|---------:|\n",
    "|35K             |~60%      |\n",
    "|50K~55K         |65~70%    |\n",
    "|45~50M          |20%       |\n",
    "|55M             |25%       |\n",
    "|75M             |Overfit   |\n",
    "\n",
    "3. Data: Breath\n",
    "\n",
    "| Parameter size | Err  |\n",
    "|:---------------|-----:|\n",
    "|45~50M          |15%   |\n",
    "|55M             |25%   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Session\n",
    "~~1. run breath~~\n",
    "\n",
    "~~2. finish tdnn~~\n",
    "\n",
    "~~* save & reload model~~\n",
    "\n",
    "~~3. examine code~~\n",
    "\n",
    "* save results: errs, epochs, configs, optimizers...\n",
    "\n",
    "* and load and show training info \n",
    "\n",
    "* Unseen speakers: put on dev and test sets\n",
    "\n",
    "~~* move layers to a seperate file~~\n",
    "\n",
    "~~4. tune: structure, params, optimizers~~\n",
    "\n",
    "* draw logic of codes\n",
    "\n",
    "~~5. grant git access~~\n",
    "\n",
    "6. look into literature\n",
    "\n",
    "7. write-up\n",
    "\n",
    "8. setup Server\n",
    "\n",
    "9. Momentum, NAG, Adagad, Adam\n",
    "\n",
    "* weight: regularization\n",
    "\n",
    "~~* dropout & weight decay~~\n",
    "\n",
    "~~* visualize ey and breath~~\n",
    "\n",
    "~~* data normalization~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Thoughts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What lies in the HEART of Machine Learning**\n",
    ">\n",
    "> Learning theory: Probably Approximately Correct (PAC) learning framework\n",
    "> 1. Measuring the goodness of learning algorithm: loss, risk, Bayes risk, emprical risk\n",
    "> 2. Functional space and its complexity: Estimation error, Approximation error, structural risk minimization, VC theory\n",
    "> 3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dynamic network structure\n",
    "\n",
    "Why dropout or dropconnect? Useful for dynamic structure, but not good enough\n",
    "\n",
    "2. What's wrong with ReLU\n",
    "3. Why gradient? Consider function and gradient in measure space\n",
    "4. Visualize network topology\n",
    "\n",
    "If we can add a back-trace pointer to trace back the strong activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update since 1/17 meeting\n",
    "\n",
    "0. Continue tuning.\n",
    "\n",
    "1. Use max pool.\n",
    "\n",
    "    - Tried max pooling with size 2 along the frequencies. No improvement.\n",
    "    - Why?\n",
    "\n",
    "2. Put rest speakers in dev and test.\n",
    "\n",
    "    - Accuracy goes down to ~ 60% for ey.\n",
    "\n",
    "3. Use dropout after LSTM layer.\n",
    "\n",
    "    - Dropout by randomly (binomially) setting 20% of activations to 0.\n",
    "    - Use inverted dropout.\n",
    "    - Much faster convergence.\n",
    "    - Seems improvement for now. Still training.\n",
    "    - Where to dropout?\n",
    "    \n",
    "4. N-way classification v.s. binary?\n",
    "\n",
    "    - softmax to sigmoid\n",
    "    - multitask\n",
    "    - generative\n",
    "\n",
    "5. SVM on LSTM output instead of logistic regression.\n",
    "\n",
    "6. Transfer learning in current process?\n",
    "\n",
    "7. Evaluate same speakers in Ey as in Breath.\n",
    "\n",
    "8. Try less speakers with more samples\n",
    "    \n",
    "    - Generally improves the performance (of course!).\n",
    "    \n",
    "9. Mann–Whitney–Wilcoxon (MWW) Test\n",
    "\n",
    "    - U test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update since 1/23 meeting\n",
    "\n",
    "**Dedicated to improving the performance of Ey.**\n",
    "\n",
    "- Work on feature\n",
    "    - DCT\n",
    "    \n",
    "    Purpose: the similar harmonic patterns in the Ey data confuses our classifier. These similar patterns are of low variations. We try to subtract the similar patterns from the Q-spectrogram by taking the DCT along the time axis and nulling the low frequency components.\n",
    "\n",
    "    Result: Get worse. DCT takes out useful low-freq information.\n",
    "    \n",
    "    - normalize\n",
    "    \n",
    "    Purpose: center the data.\n",
    "    \n",
    "    Result: It exhibits less fluctuations at the begining stage of training. No improvement on the final performance.\n",
    "\n",
    "    - log\n",
    "    \n",
    "    Purpose: just a hunch.\n",
    "    \n",
    "    Result: Get really bad because this messes the data scale.\n",
    "    \n",
    "    - augumentation with elastic transform\n",
    "    \n",
    "    Purpose: introduce random noise and generate more data to make the training more robust.\n",
    "    \n",
    "    Result: does not converge at all.\n",
    "    \n",
    "- Work on structure\n",
    "    - small filters\n",
    "    \n",
    "    Small filters yield equivalent performance to large filters followed by smaller filters.\n",
    "    \n",
    "    Reason: equivalent reception fields.\n",
    "    \n",
    "    - additional lstm layer\n",
    "    \n",
    "    Take the sequential output from the first lstm layer and feed it into a second lstm layer.\n",
    "    \n",
    "    Reason: this might further capture the sequential information. \n",
    "    \n",
    "    Result: Get worse, and really slow.\n",
    "    \n",
    "    - dropout at the lstm input\n",
    "    \n",
    "    Result: no difference.\n",
    "    \n",
    "    - dropout at the fully connected\n",
    "    \n",
    "    Results: (ongoing)\n",
    "    \n",
    "- Work on output\n",
    "    - Open set identification\n",
    "    Put the rest speakers in `Unknown` class.\n",
    "    \n",
    "    Result: ~ 60% accuracy. Training really slow.\n",
    "    \n",
    "- Verification, significance\n",
    "\n",
    "    We want to analyze how exactly our classifier performs. On which speakers it does bad? Why does it perform bad? How to improve it?\n",
    "    \n",
    "    $H_0: $ same speaker \n",
    "    $H_1: $ not same speaker\n",
    "\n",
    "    Test statistics:\n",
    "\n",
    "    Confidence interval:\n",
    "\n",
    "    Precision, recall, F-score:\n",
    "    \n",
    "    ROC, false positive, false negative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qual preparation\n",
    "\n",
    "** 1/17 - 1/22 **\n",
    "1. Forward and backward propagation\n",
    "    - https://drive.google.com/drive/folders/0B9KsZNOgwwo_TXFxZU1GdmFQNG8?usp=sharing\n",
    "    - Discussed forward and backward propagation, cross entropy, universal approximator, generative model, etc.\n",
    "2. MFCC\n",
    "\n",
    "** 1/23 - 1/28 **\n",
    "1. Multilayer perceptron\n",
    "2. Bias and variance\n",
    "3. Linear algebra\n",
    "4. DSP\n",
    "\n",
    "Committee:\n",
    "* Gary Overett\n",
    "* Bhiksha (hard time!)\n",
    "* Marios (nice)\n",
    "* Stern\n",
    "\n",
    "~~* Kumar (very strict! Failing machine)~~\n",
    "\n",
    "* Byrun Yu (nice)\n",
    "\n",
    "~~* Aarti Singh (not sure, good at machine learning, but nice?)~~\n",
    "\n",
    "* Ashwin (fine)\n",
    "* Soummya Kar (nice)\n",
    "* Ian Lane (nice)\n",
    "\n",
    "Abstract and 3 references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Update 1/29 - 2/4 **\n",
    "\n",
    "* Continue working on elastic transform\n",
    "\n",
    "    * Tune the paramters and training. Current best: $\\alpha=15$ for scale of displacement, $\\sigma=2$ for Gaussian filter. For ey, different speakers\n",
    "    ![elastic](./drafts/images/elastic.png)\n",
    "    \n",
    "    * Seems to work better than before: ~80% -> ~85% in accuracy for Ey.\n",
    "    * 5 times data, significantly slower: one epoch for one night.\n",
    "    * But error drops more per epoch: 50% error drops for the first epoch. \n",
    "    * Only performs bad on some certain speakers. Need to figure out why.\n",
    "    \n",
    "* 2d DCT\n",
    "    * Perform 2d DCT, and then null the first 15 low frequency components in rows and cols.\n",
    "    * Gives large training errors, and does not seem to converge.\n",
    "    \n",
    "* Using gradients\n",
    "    * HOG features\n",
    "        * The histogram of oriented gradients are extracted using 9 orientations, 8-by-8 pixels per cell, 3-by-3 cells per block. For ey, different speakers.\n",
    "    ![hog_feat](./drafts/images/hog_feat.png)\n",
    "    \n",
    "    * Edge detection\n",
    "        * with Sobel operator, for ey, different speakers\n",
    "            ![edge_sobel](./drafts/images/edge_sobel.png)\n",
    "    \n",
    "* Finish the first draft of Qual paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Courses\n",
    "\n",
    "1. Statistical machine learning\n",
    "2. Graphical models\n",
    "3. Signal and systems\n",
    "4. DSP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update 2/4 - 2/11**\n",
    "\n",
    "1. Tried to find out the specific set of speakers that are often wrongly classified. Remove / substitute these speakers to improve classification accuracy.\n",
    "\n",
    "2. Read papers and selected 3 reference papers.\n",
    "\n",
    "3. Revised qual paper.\n",
    "\n",
    "4. Prepared qual slides.\n",
    "\n",
    "5. Got stuck for a while on PGM assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Update 2/19 - 2/25\n",
    "\n",
    "1. Interspeech related\n",
    "    1. Try out this similar framework\n",
    "    \n",
    "    http://yerevann.github.io/2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/\n",
    "    \n",
    "    This framework was suggested by Prof. Ming, and his lab proved that it is very effective.\n",
    "    \n",
    "        1. more convolutional layers (Alex-like)\n",
    "        2. Batch normalize\n",
    "        3. GRU\n",
    "    2. Transfer learning\n",
    "    \n",
    "        1. Use out-of-set speakers to pre-train, like UBM?\n",
    "        2. fix the pre-train layer, and just update the ~50 speakers.\n",
    "        3. Extrame learning machine?\n",
    "    3. Compare to GMM-UBM\n",
    "    5. Check the frequency band of breath, and might want more bins in high frequencies; fuse const-q and dft.\n",
    "    6. write up\n",
    "2. Microfeature related\n",
    "    1. Check codes\n",
    "    2. Read literature\n",
    "    3. Approaches\n",
    "        1. ROI: region of interest\n",
    "        2. Frequency of interest\n",
    "        3. Template alphabet\n",
    "        4. sparse model (***Richard G. Baraniuk***)\n",
    "2. Qual related\n",
    "    1. paper\n",
    "    2. slides\n",
    "    3. almost finished topics\n",
    "3. Others\n",
    "    1. Setup server and environment\n",
    "    2. 702 project topic: Risk Analysis for Structured Prediction Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update 2/26 - 3/4\n",
    "\n",
    "1. Interspeech\n",
    "    1. write up https://www.sharelatex.com/project/58b60df66901f8d92ba0cce7\n",
    "    2. run some results for paper\n",
    "    3. Acc increases again (~90%)\n",
    "2. Microfeature\n",
    "    1. extract spectrograms and mfccs\n",
    "    2. write code for computing gradient features\n",
    "    3. write code for computing sparse models\n",
    "    4. TIDIGITs (ming's scripts, parallel)    \n",
    "* VOT\n",
    "\n",
    "    RNN: `Automatic Measurement of Voice Onset Time and Prevoicing using Recurrent Neural Networks` Yossi Adi1, Joseph Keshet, Olga Dmitrieva, and Matt Goldrick    \n",
    "3. Qual\n",
    "    1. finished all topics\n",
    "    2. revising slides \n",
    "4. Homework\n",
    "    1. did a tedious machine learning homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
